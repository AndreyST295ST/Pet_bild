from .deps import *

# –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏



def setup_directories():
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    main_dir = "results/–†–µ–∑—É–ª—å—Ç–∞—Ç—ã 4 –≥–ª–∞–≤—ã –∞–Ω–∞–ª–∏–∑–∞"
    analysis_dir = os.path.join(main_dir, "4.1. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ TF-IDF –∞–Ω–∞–ª–∏–∑–∞")
    
    os.makedirs(main_dir, exist_ok=True)
    os.makedirs(analysis_dir, exist_ok=True)
    
    print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {main_dir}")
    print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {analysis_dir}")
    
    return main_dir, analysis_dir

def setup_russian_analysis():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    try:
        russian_stopwords = stopwords.words('russian')
    except:
        print("–í–Ω–∏–º–∞–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ nltk. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫.")
        russian_stopwords = ['–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É']
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä–∞
    morph = pymorphy3.MorphAnalyzer()
    
    return russian_stopwords, morph

def preprocess_text(text, stopwords_list, morph_analyzer):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏,
    —á–∏—Å–µ–ª, —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è.
    """
    if pd.isna(text):
        return ""
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()
    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
    text = re.sub(f'[{string.punctuation}¬´¬ª‚Äî‚Ä¶"",,,""]', ' ', text)
    text = re.sub(r'\d+', '', text)
    # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = text.split()
    # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
    processed_words = []
    for word in words:
        if word not in stopwords_list and len(word) > 2:
            parsed_word = morph_analyzer.parse(word)[0]
            lemma = parsed_word.normal_form
            processed_words.append(lemma)
    
    return " ".join(processed_words)

def load_and_prepare_data(lost_file, found_file):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–≤—É—Ö —Ñ–∞–π–ª–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –∏ —Å–æ–∑–¥–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é is_success.
    """
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_lost = pd.read_csv(lost_file)
    df_found = pd.read_csv(found_file)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É —Ç–∏–ø–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
    df_lost['–æ–±—ä—è–≤–ª–µ–Ω–∏–µ_—Ç–∏–ø'] = 'lost'
    df_found['–æ–±—ä—è–≤–ª–µ–Ω–∏–µ_—Ç–∏–ø'] = 'found'
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    df_combined = pd.concat([df_lost, df_found], ignore_index=True)
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é is_success
    success_conditions = (
        (df_combined['—Å—Ç–∞—Ç—É—Å'] == '–ø–∏—Ç–æ–º–µ—Ü –Ω–∞–π–¥–µ–Ω') | 
        (df_combined['—Å—Ç–∞—Ç—É—Å'] == '—Ö–æ–∑—è–∏–Ω –Ω–∞–π–¥–µ–Ω')
    )
    df_combined['is_success'] = success_conditions
    
    print(f"–í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(df_combined)}")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤: {df_combined['is_success'].sum()}")
    print(f"–ù–µ—É—Å–ø–µ—à–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤: {len(df_combined) - df_combined['is_success'].sum()}")
    print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö: {df_combined['is_success'].mean():.1%}")
    
    return df_combined

def analyze_word_frequencies(df, stopwords_list, morph_analyzer):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö.
    """
    print("\n–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
    df['–æ–ø–∏—Å–∞–Ω–∏–µ_–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ'] = df['–æ–ø–∏—Å–∞–Ω–∏–µ'].apply(
        lambda x: preprocess_text(x, stopwords_list, morph_analyzer)
    )
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —É—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ
    success_texts = df[df['is_success'] == True]['–æ–ø–∏—Å–∞–Ω–∏–µ_–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ']
    fail_texts = df[df['is_success'] == False]['–æ–ø–∏—Å–∞–Ω–∏–µ_–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ']
    
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º: {success_texts.str.len().gt(0).sum()}")
    print(f"–ù–µ—É—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º: {fail_texts.str.len().gt(0).sum()}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
    all_success_words = ' '.join(success_texts).split()
    all_fail_words = ' '.join(fail_texts).split()
    
    # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
    success_freq = Counter(all_success_words)
    fail_freq = Counter(all_fail_words)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    all_words = set(all_success_words + all_fail_words)
    word_comparison = []
    
    for word in all_words:
        if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            success_count = success_freq.get(word, 0)
            fail_count = fail_freq.get(word, 0)
            total_success = len(all_success_words)
            total_fail = len(all_fail_words)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã (–Ω–∞ 1000 —Å–ª–æ–≤)
            success_rel = (success_count / total_success * 1000) if total_success > 0 else 0
            fail_rel = (fail_count / total_fail * 1000) if total_fail > 0 else 0
            
            word_comparison.append({
                'word': word,
                'success_count': success_count,
                'fail_count': fail_count,
                'success_freq_per_1000': success_rel,
                'fail_freq_per_1000': fail_rel,
                'freq_difference': success_rel - fail_rel
            })
    
    word_df = pd.DataFrame(word_comparison)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞—Å—Ç–æ
    min_occurrences = 10
    word_df = word_df[
        (word_df['success_count'] >= min_occurrences) | 
        (word_df['fail_count'] >= min_occurrences)
    ]
    
    return word_df, success_texts, fail_texts

def analyze_with_tfidf(df, success_texts, fail_texts):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é TF-IDF –ø–æ–¥—Ö–æ–¥–∞.
    """
    print("\n–ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é TF-IDF...")
    
    # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä
    vectorizer = TfidfVectorizer(
        max_features=1500, 
        min_df=5, 
        max_df=0.8,
        ngram_range=(1, 2)  # –£—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –±–∏–≥—Ä–∞–º–º—ã
    )
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º
    all_texts = df['–æ–ø–∏—Å–∞–Ω–∏–µ_–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ']
    X = vectorizer.fit_transform(all_texts)
    feature_names = vectorizer.get_feature_names_out()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —É—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
    success_idx = df[df['is_success'] == True].index
    fail_idx = df[df['is_success'] == False].index
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
    success_tfidf = X[success_idx].mean(axis=0).A1
    fail_tfidf = X[fail_idx].mean(axis=0).A1
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    tfidf_comparison = pd.DataFrame({
        'word': feature_names,
        'success_tfidf': success_tfidf,
        'fail_tfidf': fail_tfidf
    })
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –≤–∞–∂–Ω–æ—Å—Ç—å
    tfidf_comparison['tfidf_difference'] = tfidf_comparison['success_tfidf'] - tfidf_comparison['fail_tfidf']
    tfidf_comparison['abs_difference'] = abs(tfidf_comparison['tfidf_difference'])
    
    return tfidf_comparison

def visualize_results(word_df, tfidf_df, main_dir):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞.
    """
    print("\n–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    # –¢–æ–ø-20 —Å–ª–æ–≤ –ø–æ —Ä–∞–∑–Ω–∏—Ü–µ —á–∞—Å—Ç–æ—Ç
    top_success_freq = word_df.nlargest(20, 'freq_difference')
    top_fail_freq = word_df.nsmallest(20, 'freq_difference')
    
    # –¢–æ–ø-20 —Å–ª–æ–≤ –ø–æ —Ä–∞–∑–Ω–∏—Ü–µ TF-IDF
    top_success_tfidf = tfidf_df.nlargest(20, 'tfidf_difference')
    top_fail_tfidf = tfidf_df.nsmallest(20, 'tfidf_difference')
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å 4 —Å—É–±–ø–ª–æ–≥–∞–º–∏
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # 1. –°–ª–æ–≤–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ - —É—Å–ø–µ—à–Ω—ã–µ
    axes[0, 0].barh(top_success_freq['word'], top_success_freq['freq_difference'], 
                   color='lightgreen', edgecolor='darkgreen')
    axes[0, 0].set_title('–¢–æ–ø-20 —Å–ª–æ–≤: –≤ –£–°–ü–ï–®–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö\n(–ø–æ —Ä–∞–∑–Ω–∏—Ü–µ —á–∞—Å—Ç–æ—Ç)', 
                         fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('–†–∞–∑–Ω–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç (–Ω–∞ 1000 —Å–ª–æ–≤)')
    axes[0, 0].grid(axis='x', alpha=0.3)
    
    # 2. –°–ª–æ–≤–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ - –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ
    axes[0, 1].barh(top_fail_freq['word'], top_fail_freq['freq_difference'], 
                   color='lightcoral', edgecolor='darkred')
    axes[0, 1].set_title('–¢–æ–ø-20 —Å–ª–æ–≤: –≤ –ù–ï–£–°–ü–ï–®–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö\n(–ø–æ —Ä–∞–∑–Ω–∏—Ü–µ —á–∞—Å—Ç–æ—Ç)', 
                         fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('–†–∞–∑–Ω–∏—Ü–∞ —á–∞—Å—Ç–æ—Ç (–Ω–∞ 1000 —Å–ª–æ–≤)')
    axes[0, 1].grid(axis='x', alpha=0.3)
    
    # 3. –°–ª–æ–≤–∞ –ø–æ TF-IDF - —É—Å–ø–µ—à–Ω—ã–µ
    axes[1, 0].barh(top_success_tfidf['word'], top_success_tfidf['tfidf_difference'], 
                   color='lightblue', edgecolor='darkblue')
    axes[1, 0].set_title('–¢–æ–ø-20 —Å–ª–æ–≤: –≤ –£–°–ü–ï–®–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö\n(–ø–æ —Ä–∞–∑–Ω–∏—Ü–µ TF-IDF)', 
                         fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('–†–∞–∑–Ω–∏—Ü–∞ TF-IDF')
    axes[1, 0].grid(axis='x', alpha=0.3)
    
    # 4. –°–ª–æ–≤–∞ –ø–æ TF-IDF - –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ
    axes[1, 1].barh(top_fail_tfidf['word'], top_fail_tfidf['tfidf_difference'], 
                   color='orange', edgecolor='darkorange')
    axes[1, 1].set_title('–¢–æ–ø-20 —Å–ª–æ–≤: –≤ –ù–ï–£–°–ü–ï–®–ù–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö\n(–ø–æ —Ä–∞–∑–Ω–∏—Ü–µ TF-IDF)', 
                         fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('–†–∞–∑–Ω–∏—Ü–∞ TF-IDF')
    axes[1, 1].grid(axis='x', alpha=0.3)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—É—é –ø–∞–ø–∫—É
    plt.savefig(os.path.join(main_dir, '4.1.1. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ø 20 —Å–ª–æ–≤ –¥–ª—è –¥–≤—É—Ö –≤–∏–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–ø-10 —Å–ª–æ–≤
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –¥–ª—è —Ç–æ–ø-10 —É—Å–ø–µ—à–Ω—ã—Ö —Å–ª–æ–≤
    top_10_success = top_success_freq.head(10)
    x = range(len(top_10_success))
    width = 0.35
    
    ax1.bar([i - width/2 for i in x], top_10_success['success_freq_per_1000'], 
            width, label='–£—Å–ø–µ—à–Ω—ã–µ', color='green', alpha=0.7)
    ax1.bar([i + width/2 for i in x], top_10_success['fail_freq_per_1000'], 
            width, label='–ù–µ—É—Å–ø–µ—à–Ω—ã–µ', color='red', alpha=0.7)
    ax1.set_xlabel('–°–ª–æ–≤–∞')
    ax1.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (–Ω–∞ 1000 —Å–ª–æ–≤)')
    ax1.set_title('–¢–æ–ø-10 —Å–ª–æ–≤ –∏–∑ —É—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π:\n—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏')
    ax1.set_xticks(x)
    ax1.set_xticklabels(top_10_success['word'], rotation=45, ha='right')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –¥–ª—è —Ç–æ–ø-10 –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö —Å–ª–æ–≤
    top_10_fail = top_fail_freq.head(10)
    x = range(len(top_10_fail))
    
    ax2.bar([i - width/2 for i in x], top_10_fail['success_freq_per_1000'], 
            width, label='–£—Å–ø–µ—à–Ω—ã–µ', color='green', alpha=0.7)
    ax2.bar([i + width/2 for i in x], top_10_fail['fail_freq_per_1000'], 
            width, label='–ù–µ—É—Å–ø–µ—à–Ω—ã–µ', color='red', alpha=0.7)
    ax2.set_xlabel('–°–ª–æ–≤–∞')
    ax2.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (–Ω–∞ 1000 —Å–ª–æ–≤)')
    ax2.set_title('–¢–æ–ø-10 —Å–ª–æ–≤ –∏–∑ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π:\n—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏')
    ax2.set_xticks(x)
    ax2.set_xticklabels(top_10_fail['word'], rotation=45, ha='right')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—É—é –ø–∞–ø–∫—É
    plt.savefig(os.path.join(main_dir, '4.1.2. –°—Ä–∞–≤–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç –¥–ª—è —Ç–æ–ø 10 —Å–ª–æ–≤ —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    return top_success_freq, top_fail_freq, top_success_tfidf, top_fail_tfidf

def print_insights(success_words_freq, fail_words_freq, success_words_tfidf, fail_words_tfidf):
    """
    –í—ã–≤–æ–¥–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞.
    """
    print("\n" + "="*80)
    print("–ö–õ–Æ–ß–ï–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –õ–ò–ù–ì–í–ò–°–¢–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("="*80)
    
    print("\nüìà –°–õ–û–í–ê, –°–í–Ø–ó–ê–ù–ù–´–ï –° –£–°–ü–ï–•–û–ú:")
    print("–ü–æ —á–∞—Å—Ç–æ—Ç–µ:")
    for i, (_, row) in enumerate(success_words_freq.head(10).iterrows(), 1):
        print(f"  {i:2d}. {row['word']:15} (—Ä–∞–∑–Ω–∏—Ü–∞: {row['freq_difference']:+.2f})")
    
    print("\n–ü–æ TF-IDF (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞):")
    for i, (_, row) in enumerate(success_words_tfidf.head(10).iterrows(), 1):
        print(f"  {i:2d}. {row['word']:15} (—Ä–∞–∑–Ω–∏—Ü–∞ TF-IDF: {row['tfidf_difference']:+.4f})")
    
    print("\nüìâ –°–õ–û–í–ê, –°–í–Ø–ó–ê–ù–ù–´–ï –° –ù–ï–£–î–ê–ß–ï–ô:")
    print("–ü–æ —á–∞—Å—Ç–æ—Ç–µ:")
    for i, (_, row) in enumerate(fail_words_freq.head(10).iterrows(), 1):
        print(f"  {i:2d}. {row['word']:15} (—Ä–∞–∑–Ω–∏—Ü–∞: {row['freq_difference']:+.2f})")
    
    print("\n–ü–æ TF-IDF (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞):")
    for i, (_, row) in enumerate(fail_words_tfidf.head(10).iterrows(), 1):
        print(f"  {i:2d}. {row['word']:15} (—Ä–∞–∑–Ω–∏—Ü–∞ TF-IDF: {row['tfidf_difference']:+.4f})")

def step_4_1():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    plt.rcParams['font.family'] = 'DejaVu Sans'
    sns.set_palette("husl")
    pd.set_option('display.max_columns', None)


    # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º
    LOST_FILE = 'data/Dataset_final_Pet911_lost.csv'
    FOUND_FILE = 'data/dataset_final_Pet911_found.csv'
    
    try:
        print("=== –õ–ò–ù–ì–í–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –û–ü–ò–°–ê–ù–ò–ô ===")
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞...")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
        main_dir, analysis_dir = setup_directories()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        stopwords_list, morph_analyzer = setup_russian_analysis()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = load_and_prepare_data(LOST_FILE, FOUND_FILE)
        
        # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç —Å–ª–æ–≤
        word_freq_df, success_texts, fail_texts = analyze_word_frequencies(
            df, stopwords_list, morph_analyzer
        )
        
        # –ê–Ω–∞–ª–∏–∑ TF-IDF
        tfidf_df = analyze_with_tfidf(df, success_texts, fail_texts)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        success_freq, fail_freq, success_tfidf, fail_tfidf = visualize_results(
            word_freq_df, tfidf_df, main_dir
        )
        
        # –í—ã–≤–æ–¥ –∏–Ω—Å–∞–π—Ç–æ–≤
        print_insights(success_freq, fail_freq, success_tfidf, fail_tfidf)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–ø–∫—É –∞–Ω–∞–ª–∏–∑–∞
        word_freq_df.to_csv(os.path.join(analysis_dir, 'word_frequency_analysis.csv'), 
                           index=False, encoding='utf-8-sig')
        tfidf_df.to_csv(os.path.join(analysis_dir, 'tfidf_analysis.csv'), 
                       index=False, encoding='utf-8-sig')
        
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/–†–µ–∑—É–ª—å—Ç–∞—Ç—ã 4 –≥–ª–∞–≤—ã –∞–Ω–∞–ª–∏–∑–∞'")

        
    except Exception as e:
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
